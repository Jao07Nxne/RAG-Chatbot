"""
Document Processor for Thai RAG Chatbot
‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Dynamic Chunking Strategy
"""

import os
import re
from typing import List, Dict, Any
from pathlib import Path
import chardet

# Document reading libraries
import PyPDF2
from docx import Document
from pptx import Presentation

# Thai language processing
import pythainlp
from pythainlp.tokenize import word_tokenize
from pythainlp.corpus.common import thai_words

# LangChain text processing
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document as LangChainDocument

# Dynamic Chunking
from dynamic_text_splitter import DynamicTextSplitter
from content_classifier import ContentType


class ThaiDocumentProcessor:
    """‡∏Ñ‡∏•‡∏≤‡∏™‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡∏û‡∏£‡πâ‡∏≠‡∏° Dynamic Chunking"""
    
    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200, use_dynamic_chunking: bool = True):
        """
        Args:
            chunk_size: ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ chunk (characters) - ‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠ use_dynamic_chunking=False
            chunk_overlap: ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≠‡∏ô‡∏ó‡∏±‡∏ö‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á chunks - ‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠ use_dynamic_chunking=False
            use_dynamic_chunking: ‡πÉ‡∏ä‡πâ Dynamic Chunking Strategy ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ True)
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.use_dynamic_chunking = use_dynamic_chunking
        
        if use_dynamic_chunking:
            # ‡πÉ‡∏ä‡πâ Dynamic Text Splitter (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥!)
            print("‚ú® ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Dynamic Chunking Strategy")
            self.dynamic_splitter = DynamicTextSplitter()
            self.text_splitter = None  # ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ fixed splitter
        else:
            # ‡πÉ‡∏ä‡πâ Fixed Text Splitter (‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°)
            print(f"‚ö†Ô∏è ‡πÉ‡∏ä‡πâ Fixed Chunking ({chunk_size}/{chunk_overlap})")
            self.dynamic_splitter = None
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                length_function=len,
                separators=[
                    "\n\n3.1.4",   # üî• ‡∏à‡∏±‡∏ö section "‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤" ‡πÉ‡∏´‡πâ‡πÅ‡∏¢‡∏Å chunk
                    "\n\n3.1.5",   # ‡∏à‡∏±‡∏ö section ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                    "\n\n‡∏õ‡∏µ‡∏ó‡∏µ‡πà ",   # ‡∏à‡∏±‡∏ö‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏õ‡∏µ (‡πÄ‡∏ä‡πà‡∏ô "‡∏õ‡∏µ‡∏ó‡∏µ‡πà 1", "‡∏õ‡∏µ‡∏ó‡∏µ‡πà 2")
                    "\n\n‡∏†‡∏≤‡∏Ñ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏µ‡πà ",  # ‡∏à‡∏±‡∏ö‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏†‡∏≤‡∏Ñ
                    "\n\n\n",      # Multiple paragraph breaks
                    "\n\n",        # Paragraph breaks
                    "\n",          # Line breaks
                    "„ÄÇ",          # Full stop (Thai/Asian)
                    ".",           # Sentence endings
                    "!",           # Exclamation
                    "?",           # Question
                    ";",           # Semicolon
                    ",",           # Comma
                    " ",           # Spaces
                    ""             # Characters (‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)
                ]
            )
    
    def detect_encoding(self, file_path: str) -> str:
        """‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö encoding ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå"""
        try:
            with open(file_path, 'rb') as file:
                raw_data = file.read()
                result = chardet.detect(raw_data)
                return result['encoding'] or 'utf-8'
        except Exception:
            return 'utf-8'
    
    def read_text_file(self, file_path: str) -> str:
        """‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå text ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤"""
        encoding = self.detect_encoding(file_path)
        try:
            with open(file_path, 'r', encoding=encoding) as file:
                return file.read()
        except UnicodeDecodeError:
            # ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ encoding ‡∏≠‡∏∑‡πà‡∏ô
            for enc in ['utf-8', 'cp874', 'iso-8859-11']:
                try:
                    with open(file_path, 'r', encoding=enc) as file:
                        return file.read()
                except UnicodeDecodeError:
                    continue
            raise ValueError(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå {file_path} ‡πÑ‡∏î‡πâ")
    
    def read_pdf_file(self, file_path: str) -> str:
        """‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå PDF ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
        text = ""
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page_num, page in enumerate(pdf_reader.pages):
                    page_text = page.extract_text()
                    
                    # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÑ‡∏ó‡∏¢ (‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á PDF)
                    # "‡∏£ ‡πâ ‡∏≤ ‡∏ô ‡∏á ‡∏≤ ‡∏ô" ‚Üí "‡∏£‡πâ‡∏≤‡∏ô‡∏á‡∏≤‡∏ô"
                    page_text = re.sub(r'([\u0E00-\u0E7F])\s+([\u0E00-\u0E7F])', r'\1\2', page_text)
                    
                    # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÑ‡∏ó‡∏¢‡∏Å‡∏±‡∏ö‡∏™‡∏£‡∏∞/‡∏ß‡∏£‡∏£‡∏ì‡∏¢‡∏∏‡∏Å‡∏ï‡πå
                    # "‡∏ô ‡πâ ‡∏≠ ‡∏¢" ‚Üí "‡∏ô‡πâ‡∏≠‡∏¢"
                    page_text = re.sub(r'([\u0E00-\u0E7F])\s+([\u0E31-\u0E3A\u0E47-\u0E4E])', r'\1\2', page_text)
                    
                    # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏£‡∏∞‡∏ö‡∏ô/‡∏•‡πà‡∏≤‡∏á
                    page_text = re.sub(r'\s+([\u0E31-\u0E3A\u0E47-\u0E4E])', r'\1', page_text)
                    
                    text += page_text + "\n"
                    
                    # Debug: ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ
                    if page_num == 0:
                        print(f"üìÑ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏ô‡πâ‡∏≤ 1: {page_text[:200]}")
            
            return text
        except Exception as e:
            raise ValueError(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå PDF {file_path}: {str(e)}")
    
    def read_docx_file(self, file_path: str) -> str:
        """‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå Word (.docx)"""
        try:
            doc = Document(file_path)
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            return text
        except Exception as e:
            raise ValueError(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå Word {file_path}: {str(e)}")
    
    def read_pptx_file(self, file_path: str) -> str:
        """‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå PowerPoint (.pptx)"""
        try:
            prs = Presentation(file_path)
            text = ""
            for slide in prs.slides:
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        text += shape.text + "\n"
            return text
        except Exception as e:
            raise ValueError(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå PowerPoint {file_path}: {str(e)}")
    
    def clean_thai_text(self, text: str) -> str:
        """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PDF ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤)"""
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÑ‡∏ó‡∏¢
        # "‡∏Å ‡∏ß ‡∏≤ ‡∏¢" ‚Üí "‡∏Å‡∏ß‡∏≤‡∏¢"
        text = re.sub(r'([\u0E00-\u0E7F])\s+([\u0E00-\u0E7F])', r'\1\2', text)
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏Å‡∏±‡∏ö‡∏™‡∏£‡∏∞/‡∏ß‡∏£‡∏£‡∏ì‡∏¢‡∏∏‡∏Å‡∏ï‡πå
        # "‡∏ô ‡πâ ‡∏≠ ‡∏¢" ‚Üí "‡∏ô‡πâ‡∏≠‡∏¢", "‡∏£ ‡∏± ‡∏ö" ‚Üí "‡∏£‡∏±‡∏ö"
        text = re.sub(r'([\u0E00-\u0E7F])\s+([\u0E31-\u0E3A\u0E47-\u0E4E])', r'\1\2', text)
        text = re.sub(r'\s+([\u0E31-\u0E3A\u0E47-\u0E4E])\s+', r'\1', text)
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: üî• ‡πÅ‡∏Å‡πâ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÉ‡∏ô‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤ ‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏Å‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏≠‡∏∑‡πà‡∏ô
        # "0550 6231" ‚Üí "05506231", "0550  6231" ‚Üí "05506231"
        text = re.sub(r'\b(\d{4})\s+(\d{4})\b', r'\1\2', text)
        text = re.sub(r'\b(\d{2})\s+(\d{2})\s+(\d{4})\b', r'\1\2\3', text)
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 4: ‡πÅ‡∏Å‡πâ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç/‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏Ñ‡∏≥‡πÑ‡∏ó‡∏¢
        text = re.sub(r'‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£4', '‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå', text)
        text = re.sub(r'([\u0E00-\u0E7F]+)4', r'\1‡πå', text)  # ‡πÅ‡∏õ‡∏•‡∏á "‡∏Ñ‡∏≥4" ‚Üí "‡∏Ñ‡∏≥‡πå"
        text = re.sub(r'([\u0E00-\u0E7F]+)e', r'\1‡∏µ', text)  # ‡πÅ‡∏õ‡∏•‡∏á "‡∏õe" ‚Üí "‡∏õ‡∏µ"
        text = re.sub(r'([\u0E00-\u0E7F]+)F', r'\1‡πâ', text)  # ‡πÅ‡∏õ‡∏•‡∏á "‡∏´‡∏ôF" ‚Üí "‡∏´‡∏ô‡πâ"
        text = re.sub(r'([\u0E00-\u0E7F]+)Q', r'\1‡πâ', text)  # ‡πÅ‡∏õ‡∏•‡∏á "‡∏ÅQ" ‚Üí "‡∏Å‡πâ"
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
        text = re.sub(r'‡πÄ‡∏õb‡∏î', '‡πÄ‡∏õ‡∏¥‡∏î', text)
        text = re.sub(r'‡πÑ‡∏°‡πâ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πâ‡∏≤', '‡πÑ‡∏°‡πà‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤', text)
        text = re.sub(r'‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πâ', '‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà', text)
        text = re.sub(r'‡∏ù√•', '‡∏ù‡∏∂', text)
        text = re.sub(r'‡∏û‡∏£‡∏∞‡∏à‡∏≠‡∏°‡πÄ‡∏Å‡∏•<‡∏≤‡πÄ‡∏à<‡∏≤', '‡∏û‡∏£‡∏∞‡∏à‡∏≠‡∏°‡πÄ‡∏Å‡∏•‡πâ‡∏≤‡πÄ‡∏à‡πâ‡∏≤', text)
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 5: ‡πÅ‡∏Å‡πâ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏±‡∏Å‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô PDF
        common_fixes = {
            r'‡∏´ ‡∏ô ‡πà ‡∏ß ‡∏¢': '‡∏´‡∏ô‡πà‡∏ß‡∏¢',
            r'‡∏´ ‡∏ô ‡πà ‡∏ß‡∏¢': '‡∏´‡∏ô‡πà‡∏ß‡∏¢',
            r'‡∏´‡∏ô‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï': '‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï',
            r'‡∏Å ‡∏¥ ‡∏ï': '‡∏Å‡∏¥‡∏ï',
            r'‡∏Å ‡∏¥‡∏ï': '‡∏Å‡∏¥‡∏ï',
            r'‡∏´ ‡∏• ‡∏± ‡∏Å ‡∏™ ‡∏π ‡∏ï ‡∏£': '‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£',
            r'‡∏´ ‡∏• ‡∏± ‡∏Å‡∏™ ‡∏π ‡∏ï‡∏£': '‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£',
            r'‡∏ß ‡∏¥ ‡∏ó ‡∏¢ ‡∏≤': '‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤',
            r'‡∏ß ‡∏¥‡∏ó‡∏¢ ‡∏≤': '‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤',
            r'‡∏® ‡∏∂ ‡∏Å ‡∏© ‡∏≤': '‡∏®‡∏∂‡∏Å‡∏©‡∏≤',
            r'‡∏® ‡∏∂‡∏Å‡∏© ‡∏≤': '‡∏®‡∏∂‡∏Å‡∏©‡∏≤',
            r'‡∏† ‡∏≤ ‡∏Ñ': '‡∏†‡∏≤‡∏Ñ',
            r'‡πÄ‡∏ó ‡∏≠ ‡∏°': '‡πÄ‡∏ó‡∏≠‡∏°',
            r'‡∏õ ‡∏£ ‡∏∞ ‡∏à ‡∏≥': '‡∏õ‡∏£‡∏∞‡∏à‡∏≥',
            r'‡πÄ‡∏Å‡∏•Q‡∏≤‡πÄ‡∏àQ‡∏≤': '‡πÄ‡∏Å‡∏•‡πâ‡∏≤‡πÄ‡∏à‡πâ‡∏≤',
            r'‡∏ßF‡∏≤': '‡∏ß‡πà‡∏≤',
            r'‡∏îQ‡∏ß‡∏¢': '‡∏î‡πâ‡∏ß‡∏¢',
            # üî• ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö
            r'‡∏Ñ‡∏î‡∏µ‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏î‡∏¥‡∏•‡∏¥‡∏•‡∏ò‡∏¥‡∏•': '‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏î‡∏¥‡∏™‡∏Ñ‡∏£‡∏µ‡∏ï',
            r'‡∏Ñ‡∏î‡∏µ‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå': '‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå',
            r'‡∏î‡∏¥‡∏•‡∏¥‡∏•‡∏ò‡∏¥‡∏•': '‡∏î‡∏¥‡∏™‡∏Ñ‡∏£‡∏µ‡∏ï',
            r'‡∏õ‡∏è‡∏¥‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏£‡∏∞': '‡∏õ‡∏è‡∏¥‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå',
            r'‡∏™‡∏ß‡∏±‡∏î‡∏Å‡∏£‡∏£‡∏°': '‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°',
            r'‡∏™‡∏ß‡∏±‡∏î': '‡∏ß‡∏¥‡∏®‡∏ß',
        }
        
        for pattern, replacement in common_fixes.items():
            text = re.sub(pattern, replacement, text)
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 6: ‡∏•‡∏ö whitespace ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô (‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 1 ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á)
        text = re.sub(r' {2,}', ' ', text)
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 7: ‡∏•‡∏ö‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (‡πÅ‡∏ï‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç)
        text = re.sub(r'[^\u0E00-\u0E7F\w\s\.\,\!\?\;\:\-\(\)\"\'\/\n]', '', text)
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 8: ‡∏•‡∏ö‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ß‡πà‡∏≤‡∏á‡∏ã‡πâ‡∏≥ (‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 2 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î)
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 9: ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ï‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏ó‡πâ‡∏≤‡∏¢‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
        text = '\n'.join(line.strip() for line in text.split('\n'))
        
        return text.strip()
    
    def preprocess_text(self, text: str) -> str:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á chunks"""
        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
        text = self.clean_thai_text(text)
        
        # ‡∏ï‡∏±‡∏î word ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á chunk
        # (‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏Å‡πá‡πÑ‡∏î‡πâ ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)
        
        return text
    
    def read_document(self, file_path: str) -> str:
        """‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå"""
        file_path = Path(file_path)
        file_extension = file_path.suffix.lower()
        
        if file_extension == '.txt':
            return self.read_text_file(str(file_path))
        elif file_extension == '.pdf':
            return self.read_pdf_file(str(file_path))
        elif file_extension in ['.docx', '.doc']:
            return self.read_docx_file(str(file_path))
        elif file_extension in ['.pptx', '.ppt']:
            return self.read_pptx_file(str(file_path))
        else:
            # ‡∏•‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô text file
            try:
                return self.read_text_file(str(file_path))
            except:
                raise ValueError(f"‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó {file_extension}")
    
    def process_document(self, file_path: str, metadata: Dict[str, Any] = None) -> List[LangChainDocument]:
        """
        ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô chunks (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Dynamic Chunking)
        
        Args:
            file_path: ‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå
            metadata: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏Ç‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
            
        Returns:
            List[LangChainDocument]: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ chunks ‡∏û‡∏£‡πâ‡∏≠‡∏° metadata
        """
        # ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
        text = self.read_document(file_path)
        
        # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        processed_text = self.preprocess_text(text)
        
        if not processed_text.strip():
            print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£")
            return []
        
        # ‡πÅ‡∏ö‡πà‡∏á chunks ‡∏ï‡∏≤‡∏° strategy
        if self.use_dynamic_chunking:
            # ‡πÉ‡∏ä‡πâ Dynamic Chunking
            print("\nüéØ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÅ‡∏ö‡πà‡∏á chunks ‡∏î‡πâ‡∏ß‡∏¢ Dynamic Strategy...")
            chunks, content_type = self.dynamic_splitter.split_text(processed_text)
        else:
            # ‡πÉ‡∏ä‡πâ Fixed Chunking (‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°)
            print(f"\nüìù ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÅ‡∏ö‡πà‡∏á chunks (Fixed: {self.chunk_size}/{self.chunk_overlap})...")
            chunks = self.text_splitter.split_text(processed_text)
            content_type = "general"  # Default
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á LangChain Documents ‡∏û‡∏£‡πâ‡∏≠‡∏° metadata ‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
        documents = []
        file_name = Path(file_path).name
        
        base_metadata = {
            "source": file_path,
            "filename": file_name,
            "total_chunks": len(chunks)
        }
        
        if metadata:
            base_metadata.update(metadata)
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° content_type ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ Dynamic Chunking
        if self.use_dynamic_chunking:
            base_metadata["content_type"] = content_type
        
        for i, chunk in enumerate(chunks):
            chunk_metadata = base_metadata.copy()
            chunk_metadata.update({
                "chunk_index": i,
                "chunk_size": len(chunk)
            })
            
            # üîç ‡πÄ‡∏û‡∏¥‡πà‡∏° metadata ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏∂‡πâ‡∏ô
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤ (8 ‡∏´‡∏•‡∏±‡∏Å ‡πÄ‡∏ä‡πà‡∏ô 05506232, 90641001)
            course_codes = re.findall(r'\b\d{8}\b', chunk)
            if course_codes:
                chunk_metadata["course_codes"] = ",".join(list(set(course_codes)))
            
            # üî• ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ä‡∏±‡πâ‡∏ô‡∏õ‡∏µ‡πÅ‡∏•‡∏∞‡∏†‡∏≤‡∏Ñ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ (‡∏õ‡∏£‡∏±‡∏ö regex ‡πÉ‡∏´‡πâ‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö)
            # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö 1: "‡∏õ‡∏µ‡∏ó‡∏µ‡πà 1" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏õ‡∏µ‡∏ó‡∏µ‡πà1"
            year_patterns = [
                r'‡∏õ‡∏µ‡∏ó‡∏µ‡πà\s*(\d+)',
                r'‡∏ä‡∏±‡πâ‡∏ô‡∏õ‡∏µ\s*(\d+)',
                r'‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤\s*‡∏õ‡∏µ‡∏ó‡∏µ‡πà\s*(\d+)',
            ]
            year_found = None
            for pattern in year_patterns:
                match = re.search(pattern, chunk)
                if match:
                    year_found = match.group(1)
                    break
            
            # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö 2: "‡∏†‡∏≤‡∏Ñ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏µ‡πà 1" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏†‡∏≤‡∏Ñ 1"
            semester_patterns = [
                r'‡∏†‡∏≤‡∏Ñ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏µ‡πà\s*(\d+)',
                r'‡∏†‡∏≤‡∏Ñ\s*(\d+)',
                r'‡πÄ‡∏ó‡∏≠‡∏°\s*(\d+)',
            ]
            semester_found = None
            for pattern in semester_patterns:
                match = re.search(pattern, chunk)
                if match:
                    semester_found = match.group(1)
                    break
            
            if year_found:
                chunk_metadata["year"] = year_found
                # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡∏õ‡∏µ‡πÅ‡∏•‡∏∞‡∏†‡∏≤‡∏Ñ ‚Üí tag ‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô curriculum table
                if semester_found:
                    chunk_metadata["semester"] = semester_found
                    chunk_metadata["is_curriculum_table"] = "yes"
                    
                    # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á
                    if course_codes:
                        chunk_metadata["course_count"] = len(course_codes)
                    
                    # ‡∏î‡∏∂‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏£‡∏ß‡∏°
                    total_match = re.search(r'‡∏£‡∏ß‡∏°\s+(\d+)\s+‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï', chunk)
                    if total_match:
                        chunk_metadata["total_credits"] = total_match.group(1)
                    
                    print(f"   üéØ ‡∏û‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£: ‡∏õ‡∏µ {year_found} ‡∏†‡∏≤‡∏Ñ {semester_found} "
                          f"({len(course_codes)} ‡∏ß‡∏¥‡∏ä‡∏≤) ‡πÉ‡∏ô chunk {i}")
            elif semester_found:
                chunk_metadata["semester"] = semester_found
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï
            credit_matches = re.findall(r'(\d+)\s*‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï', chunk)
            if credit_matches:
                chunk_metadata["has_credits"] = "yes"
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏à‡∏≤‡∏Å chunk ‡πÄ‡∏õ‡πá‡∏ô preview
            chunk_metadata["preview"] = chunk[:150].strip().replace('\n', ' ')
            
            documents.append(LangChainDocument(
                page_content=chunk,
                metadata=chunk_metadata
            ))
        
        print(f"‚úÖ ‡πÅ‡∏ö‡πà‡∏á chunks ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô: {len(documents)} chunks")
        
        return documents
    
    def process_multiple_documents(self, file_paths: List[str]) -> List[LangChainDocument]:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏•‡∏≤‡∏¢‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô"""
        all_documents = []
        
        for file_path in file_paths:
            try:
                documents = self.process_document(file_path)
                all_documents.extend(documents)
                print(f"‚úÖ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå {Path(file_path).name} ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô - {len(documents)} chunks")
            except Exception as e:
                print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå {Path(file_path).name}: {str(e)}")
        
        return all_documents
    
    def get_text_stats(self, text: str) -> Dict[str, Any]:
        """‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
        words = word_tokenize(text, engine='newmm')
        thai_words_count = len([w for w in words if re.match(r'[\u0E00-\u0E7F]+', w)])
        
        return {
            "total_characters": len(text),
            "total_words": len(words),
            "thai_words": thai_words_count,
            "lines": len(text.split('\n')),
            "estimated_chunks": (len(text) // self.chunk_size) + 1
        }


# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
if __name__ == "__main__":
    processor = ThaiDocumentProcessor(chunk_size=1000, chunk_overlap=200)
    
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå
    try:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏î‡∏™‡∏≠‡∏ö
        test_text = """
        ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö RAG Chatbot
        
        ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡πà‡∏≤ ‡∏£‡∏≤‡∏ä‡∏≠‡∏≤‡∏ì‡∏≤‡∏à‡∏±‡∏Å‡∏£‡πÑ‡∏ó‡∏¢ ‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÉ‡∏ô‡πÄ‡∏≠‡πÄ‡∏ä‡∏µ‡∏¢‡∏ï‡∏∞‡∏ß‡∏±‡∏ô‡∏≠‡∏≠‡∏Å‡πÄ‡∏â‡∏µ‡∏¢‡∏á‡πÉ‡∏ï‡πâ 
        ‡∏°‡∏µ‡∏û‡∏£‡∏°‡πÅ‡∏î‡∏ô‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ö‡∏û‡∏°‡πà‡∏≤ ‡∏•‡∏≤‡∏ß ‡∏Å‡∏±‡∏°‡∏û‡∏π‡∏ä‡∏≤ ‡πÅ‡∏•‡∏∞‡∏°‡∏≤‡πÄ‡∏•‡πÄ‡∏ã‡∏µ‡∏¢ ‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 513,120 ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏Å‡∏¥‡πÇ‡∏•‡πÄ‡∏°‡∏ï‡∏£
        
        ‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏´‡∏•‡∏ß‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢ 
        ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 70 ‡∏•‡πâ‡∏≤‡∏ô‡∏Ñ‡∏ô ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏£‡∏≤‡∏ä‡∏Å‡∏≤‡∏£
        
        ‡∏£‡∏∞‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏ä‡∏≤‡∏ò‡∏¥‡∏õ‡πÑ‡∏ï‡∏¢‡∏†‡∏≤‡∏¢‡πÉ‡∏ï‡πâ‡∏£‡∏±‡∏ò‡∏£‡∏£‡∏°‡∏ô‡∏π‡∏ç ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏û‡∏£‡∏∞‡∏°‡∏´‡∏≤‡∏Å‡∏©‡∏±‡∏ï‡∏£‡∏¥‡∏¢‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏∏‡∏Ç
        """
        
        with open("test_thai.txt", "w", encoding="utf-8") as f:
            f.write(test_text)
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
        docs = processor.process_document("test_thai.txt")
        
        print(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô chunks: {len(docs)}")
        for i, doc in enumerate(docs):
            print(f"\nChunk {i+1}:")
            print(f"Content: {doc.page_content[:100]}...")
            print(f"Metadata: {doc.metadata}")
        
        # ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏î‡∏™‡∏≠‡∏ö
        os.remove("test_thai.txt")
        
    except Exception as e:
        print(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")