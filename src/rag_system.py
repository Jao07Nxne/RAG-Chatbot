"""
RAG System for Thai Chatbot with Chat History Support
‡∏£‡∏∞‡∏ö‡∏ö RAG ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÅ‡∏ö‡∏ö‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ Local LLM
"""

import os
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

from langchain.schema import Document, BaseMessage, HumanMessage, AIMessage
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate
from langchain.llms.base import LLM
from langchain.callbacks.manager import CallbackManagerForLLMRun

import ollama
import requests


class OllamaLLM(LLM):
    """Custom LLM class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Ollama"""
    
    model_name: str = "llama3.1"
    temperature: float = 0.1
    max_tokens: int = 2048
    
    def __init__(self, model_name: str = "llama3.1", temperature: float = 0.1, **kwargs):
        super().__init__(**kwargs)
        self.model_name = model_name
        self.temperature = temperature
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Ollama
        self._check_ollama_connection()
    
    def _check_ollama_connection(self) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Ollama"""
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                available_models = [model['name'] for model in models]
                
                if self.model_name not in available_models:
                    print(f"‚ö†Ô∏è ‡πÇ‡∏°‡πÄ‡∏î‡∏• {self.model_name} ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ô Ollama")
                    print(f"üìã ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà: {available_models}")
                    if available_models:
                        print(f"üîÑ ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• {available_models[0]} ‡πÅ‡∏ó‡∏ô")
                        self.model_name = available_models[0]
                
                print(f"‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Ollama ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à - ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•: {self.model_name}")
                return True
            else:
                raise Exception(f"HTTP {response.status_code}")
                
        except Exception as e:
            print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Ollama: {e}")
            print("üí° ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Ollama ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà (ollama serve)")
            return False
    
    def _call(self, prompt: str, stop: Optional[List[str]] = None, 
              run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs) -> str:
        """‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Ollama LLM"""
        try:
            response = ollama.chat(
                model=self.model_name,
                messages=[
                    {
                        "role": "system", 
                        "content": "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏ï‡∏≠‡∏ö‡∏™‡∏±‡πâ‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡∏Ñ‡∏≥ ‡∏´‡πâ‡∏≤‡∏°‡∏û‡∏π‡∏î‡πÄ‡∏¢‡∏¥‡πà‡∏ô‡πÄ‡∏¢‡πâ‡∏≠"
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                options={
                    "temperature": self.temperature,
                    "num_predict": min(self.max_tokens, 512),  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
                    "top_p": 0.9,
                    "top_k": 40,  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î choices
                    "repeat_penalty": 1.5,  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡πà‡∏≤ penalty ‡∏Å‡∏≤‡∏£‡∏ã‡πâ‡∏≥
                    "stop": [
                        "</s>", 
                        "<|end|>", 
                        "Human:", 
                        "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:",
                        "\n\n‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°",  # ‡∏´‡∏¢‡∏∏‡∏î‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà
                        "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•",  # ‡∏´‡∏¢‡∏∏‡∏î‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏õ‡∏¥‡∏î
                        "\n\n\n"  # ‡∏´‡∏¢‡∏∏‡∏î‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ß‡πà‡∏≤‡∏á 3 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
                    ]
                }
            )
            
            result = response['message']['content']
            
            # ‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏≠‡∏≠‡∏Å (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ã‡πâ‡∏≥‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 50 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)
            lines = result.split('\n')
            seen = set()
            unique_lines = []
            for line in lines:
                line_stripped = line.strip()
                if len(line_stripped) > 20:  # ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
                    if line_stripped not in seen:
                        unique_lines.append(line)
                        seen.add(line_stripped)
                else:
                    unique_lines.append(line)
            
            result = '\n'.join(unique_lines)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
            if not result or len(result.strip()) < 5:
                return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÑ‡∏î‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á"
            
            # ‡∏ï‡∏±‡∏î‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô‡∏ñ‡πâ‡∏≤‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (max 1000 chars)
            if len(result) > 1000:
                result = result[:997] + "..."
            
            return result.strip()
            
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM: {e}")
            return f"‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•: {str(e)}"
    
    @property
    def _llm_type(self) -> str:
        return "ollama"


class ThaiRAGSystem:
    """‡∏£‡∏∞‡∏ö‡∏ö RAG ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏û‡∏£‡πâ‡∏≠‡∏° Chat History"""
    
    def __init__(self, 
                 vector_store_manager,
                 llm_model: str = "llama3.1",
                 temperature: float = 0.1,
                 max_tokens: int = 2048):
        """
        Args:
            vector_store_manager: ThaiVectorStoreManager instance
            llm_model: ‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏• Ollama ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ
            temperature: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö (0-1)
            max_tokens: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö
        """
        self.vector_store_manager = vector_store_manager
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á LLM
        self.llm = OllamaLLM(
            model_name=llm_model,
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Memory ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt Template
        self.prompt_template = self._create_prompt_template()
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á RAG Chain
        self.rag_chain = None
        self._setup_rag_chain()
    
    def _create_prompt_template(self) -> PromptTemplate:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt Template ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"""
        template = """‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥

‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£:
{context}

‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤:
{chat_history}

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}

‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö (‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ñ‡∏£‡πà‡∏á‡∏Ñ‡∏£‡∏±‡∏î):
1. ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
2. ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤ - ‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏ï‡πà‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á
3. **‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≤‡∏°‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô 05506231)** ‚Üí ‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏£‡∏´‡∏±‡∏™‡∏ô‡∏±‡πâ‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏à‡∏≠‡∏Å‡πà‡∏≠‡∏ô‡∏ï‡∏≠‡∏ö ‡∏´‡πâ‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡∏°‡∏±‡πà‡∏ß
4. **‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≤‡∏°‡∏õ‡∏µ‡∏ó‡∏µ‡πà X ‡∏†‡∏≤‡∏Ñ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏µ‡πà Y** ‚Üí ‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏õ‡∏µ/‡∏†‡∏≤‡∏Ñ‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
5. **‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≤‡∏° "‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"** ‚Üí ‡∏£‡∏∞‡∏ö‡∏∏‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏û‡∏ö
6. ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‚Üí ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•..." ‡∏´‡πâ‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏≠‡∏∑‡πà‡∏ô
7. ‡∏ï‡∏≠‡∏ö‡∏™‡∏±‡πâ‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô 3-5 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ (‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≤‡∏°‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏£‡∏ö)
8. ‡∏´‡πâ‡∏≤‡∏°‡∏ó‡∏≥‡∏ã‡πâ‡∏≥‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏´‡πâ‡∏≤‡∏°‡∏ï‡πà‡∏≠‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
9. ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ‚Üí ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏õ‡πá‡∏ô bullet points
10. ‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏£‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô

‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:"""
        
        return PromptTemplate(
            template=template,
            input_variables=["context", "chat_history", "question"]
        )
    
    def _setup_rag_chain(self):
        """‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ RAG Chain"""
        if self.vector_store_manager.vector_store is None:
            print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ vector store ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á RAG chain")
            return
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á retriever
        retriever = self.vector_store_manager.get_retriever(k=5)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á ConversationalRetrievalChain
        self.rag_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=retriever,
            memory=self.memory,
            return_source_documents=True,
            combine_docs_chain_kwargs={"prompt": self.prompt_template},
            verbose=True
        )
        
        print("‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á RAG Chain ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    
    def ask_question(self, question: str) -> Dict[str, Any]:
        """
        ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        
        Args:
            question: ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
            
        Returns:
            Dict containing answer, sources, and metadata
        """
        if self.rag_chain is None:
            return {
                "answer": "‡∏£‡∏∞‡∏ö‡∏ö‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏Å‡πà‡∏≠‡∏ô",
                "sources": [],
                "question": question,
                "timestamp": datetime.now().isoformat()
            }
        
        try:
            print(f"ü§î ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ vector store ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if not self.vector_store_manager.vector_store:
                return {
                    "answer": "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏Å‡πà‡∏≠‡∏ô",
                    "sources": [],
                    "question": question,
                    "timestamp": datetime.now().isoformat()
                }
            
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á - ‡πÄ‡∏û‡∏¥‡πà‡∏° k ‡πÄ‡∏õ‡πá‡∏ô 5 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
            relevant_docs = self.vector_store_manager.vector_store.similarity_search(
                question, 
                k=5  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 3 ‡πÄ‡∏õ‡πá‡∏ô 5 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
            )
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á context
            if not relevant_docs:
                return {
                    "answer": "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î",
                    "sources": [],
                    "question": question,
                    "timestamp": datetime.now().isoformat()
                }
            
            # üéØ ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (Metadata Filtering)
            import re
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á 05506231 ‡πÅ‡∏•‡∏∞ 0550 6231)
            question_course_code = re.search(r'\b(\d{4})\s*(\d{4})\b|\b(\d{8})\b', question)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ä‡∏±‡πâ‡∏ô‡∏õ‡∏µ/‡∏†‡∏≤‡∏Ñ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
            question_year = re.search(r'‡∏õ‡∏µ‡∏ó‡∏µ‡πà\s*(\d+)', question)
            question_semester = re.search(r'‡∏†‡∏≤‡∏Ñ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏µ‡πà\s*(\d+)', question)
            
            filtered_docs = []
            
            # ‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏° metadata ‡∏ñ‡πâ‡∏≤‡∏û‡∏ö‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
            for doc in relevant_docs:
                metadata = doc.metadata
                content = doc.page_content
                
                # ‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≤‡∏°‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‚Üí ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏£‡∏´‡∏±‡∏™‡∏ô‡∏±‡πâ‡∏ô‡πÉ‡∏ô content (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á)
                if question_course_code:
                    # ‡∏î‡∏∂‡∏á‡∏£‡∏´‡∏±‡∏™‡∏à‡∏≤‡∏Å regex (group 1+2 ‡∏´‡∏£‡∏∑‡∏≠ group 3)
                    if question_course_code.group(3):  # 8 ‡∏´‡∏•‡∏±‡∏Å‡∏ï‡∏¥‡∏î
                        target_code = question_course_code.group(3)
                    else:  # 4+4 ‡∏´‡∏•‡∏±‡∏Å‡πÅ‡∏¢‡∏Å
                        target_code = question_course_code.group(1) + question_course_code.group(2)
                    
                    # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö: "05506231" ‡∏´‡∏£‡∏∑‡∏≠ "0550 6231" ‡∏´‡∏£‡∏∑‡∏≠ "0550-6231"
                    pattern1 = target_code  # ‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô
                    pattern2 = target_code[:4] + r'\s*' + target_code[4:]  # ‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á
                    
                    if (target_code in content or 
                        re.search(pattern2, content) or
                        target_code in metadata.get("course_codes", "")):
                        filtered_docs.append(doc)
                        continue
                
                # ‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≤‡∏°‡∏õ‡∏µ/‡∏†‡∏≤‡∏Ñ ‚Üí ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö metadata (‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î!)
                elif question_year or question_semester:
                    # üî• ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏õ‡∏µ‡πÅ‡∏•‡∏∞‡∏†‡∏≤‡∏Ñ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏π‡πà)
                    if question_year and question_semester:
                        # ‡∏ï‡∏£‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏π‡πà‡∏ñ‡∏∂‡∏á‡∏à‡∏∞‡∏ú‡πà‡∏≤‡∏ô
                        if (metadata.get("year") == question_year.group(1) and 
                            metadata.get("semester") == question_semester.group(1)):
                            # Priority ‡∏™‡∏π‡∏á‡∏ñ‡πâ‡∏≤‡∏°‡∏µ tag curriculum table
                            if metadata.get("is_curriculum_table") == "yes":
                                filtered_docs.insert(0, doc)  # ‡πÉ‡∏™‡πà‡∏´‡∏ô‡πâ‡∏≤‡∏™‡∏∏‡∏î!
                            else:
                                filtered_docs.append(doc)
                            continue
                    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÅ‡∏Ñ‡πà‡∏õ‡∏µ ‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏Ñ‡πà‡∏†‡∏≤‡∏Ñ
                    elif question_year and metadata.get("year") == question_year.group(1):
                        filtered_docs.append(doc)
                        continue
                    elif question_semester and metadata.get("semester") == question_semester.group(1):
                        filtered_docs.append(doc)
                        continue
                
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏û‡∏¥‡πÄ‡∏®‡∏© ‚Üí ‡πÄ‡∏≠‡∏≤‡∏ó‡∏∏‡∏Å doc
                else:
                    filtered_docs.append(doc)
            
            # ‡∏ñ‡πâ‡∏≤‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏•‡∏¢ ‚Üí ‡πÉ‡∏ä‡πâ docs ‡πÄ‡∏î‡∏¥‡∏°
            if not filtered_docs:
                filtered_docs = relevant_docs
                print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(relevant_docs)} chunks")
            else:
                print(f"‚úÖ ‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {len(filtered_docs)} chunks ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç")
            
            # Deduplication
            unique_docs = []
            seen_contents = set()
            for doc in filtered_docs:
                # ‡πÉ‡∏ä‡πâ 100 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÅ‡∏£‡∏Å‡πÄ‡∏õ‡πá‡∏ô signature
                signature = doc.page_content[:100].strip()
                if signature not in seen_contents:
                    unique_docs.append(doc)
                    seen_contents.add(signature)
            
            # ‡πÉ‡∏ä‡πâ top 2 chunks (‡∏•‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Ollama ‡∏•‡πà‡∏°)
            relevant_docs = unique_docs[:2]
            
            # üî• ‡∏™‡∏£‡πâ‡∏≤‡∏á context ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ï‡πá‡∏° (‡πÑ‡∏°‡πà‡∏ï‡∏±‡∏î) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á
            # gemma2:2b ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö ~2048 tokens ‚âà 1500-1800 chars
            context_parts = []
            total_chars = 0
            max_context_chars = 1500  # ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö gemma2:2b
            
            for doc in relevant_docs:
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ß‡πà‡∏≤‡∏¢‡∏±‡∏á‡πÉ‡∏™‡πà‡πÑ‡∏î‡πâ‡∏≠‡∏µ‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà
                remaining = max_context_chars - total_chars
                
                if remaining <= 0:
                    break
                
                # ‡πÉ‡∏ä‡πâ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ï‡πá‡∏°‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô limit
                content = doc.page_content[:remaining]
                context_parts.append(content)
                total_chars += len(content)
            
            context = "\n\n---\n\n".join(context_parts)
            
            print(f"ÔøΩ ‡πÉ‡∏ä‡πâ context ‡∏£‡∏ß‡∏° {total_chars} chars ‡∏à‡∏≤‡∏Å {len(context_parts)} chunks")
            
            # Debug: ‡πÅ‡∏™‡∏î‡∏á context
            print(f"üìÑ Context ‡∏ó‡∏µ‡πà‡∏û‡∏ö ({len(context)} chars): {context[:200]}...")
            
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ RAG Chain ‡∏î‡πâ‡∏ß‡∏¢ ConversationalRetrievalChain
            result = self.rag_chain({
                "question": question,
                "chat_history": self.memory.chat_memory.messages
            })
            
            # ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            sources = []
            seen_sources = set()
            for doc in relevant_docs:
                source_info = {
                    "content": doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content,
                    "metadata": doc.metadata,
                    "filename": doc.metadata.get("filename", "‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö"),
                    "chunk_index": doc.metadata.get("chunk_index", 0)
                }
                
                # ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡∏ã‡πâ‡∏≥
                source_key = f"{source_info['filename']}_{source_info['chunk_index']}"
                if source_key not in seen_sources:
                    sources.append(source_info)
                    seen_sources.add(source_key)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
            answer = result.get("answer", result.get("text", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ"))
            
            # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
            if len(answer.strip()) < 10:
                answer = "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÑ‡∏î‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°"
            
            print(f"‚úÖ ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {answer[:100]}...")
            
            return {
                "answer": answer,
                "sources": sources,
                "question": question,
                "timestamp": datetime.now().isoformat(),
                "chat_history_length": len(self.memory.chat_memory.messages)
            }
            
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {e}")
            return {
                "answer": f"‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}",
                "sources": [],
                "question": question,
                "timestamp": datetime.now().isoformat()
            }
    
    def get_chat_history(self) -> List[Dict[str, str]]:
        """‡∏î‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤"""
        history = []
        messages = self.memory.chat_memory.messages
        
        for i in range(0, len(messages), 2):
            if i + 1 < len(messages):
                human_msg = messages[i]
                ai_msg = messages[i + 1]
                
                history.append({
                    "question": human_msg.content,
                    "answer": ai_msg.content,
                    "timestamp": datetime.now().isoformat()  # ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÄ‡∏Å‡πá‡∏ö timestamp ‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô metadata
                })
        
        return history
    
    def clear_chat_history(self):
        """‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤"""
        self.memory.clear()
        print("‚úÖ ‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
    
    def update_vector_store(self, vector_store_manager):
        """‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó vector store ‡πÅ‡∏•‡∏∞ RAG chain"""
        self.vector_store_manager = vector_store_manager
        self._setup_rag_chain()
        print("‚úÖ ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó vector store ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
    
    def get_system_stats(self) -> Dict[str, Any]:
        """‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö"""
        vector_stats = self.vector_store_manager.get_stats()
        
        return {
            "vector_store": vector_stats,
            "llm_model": self.llm.model_name,
            "chat_history_length": len(self.memory.chat_memory.messages) // 2,
            "rag_chain_ready": self.rag_chain is not None,
            "timestamp": datetime.now().isoformat()
        }


# ‡πÇ‡∏°‡πÄ‡∏î‡∏• LLM ‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
RECOMMENDED_THAI_LLM_MODELS = {
    "small_fast": "gemma2:2b",        # ‡πÄ‡∏•‡πá‡∏Å, ‡πÄ‡∏£‡πá‡∏ß, RAM ‡∏ô‡πâ‡∏≠‡∏¢ (4GB) - ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
    "balanced": "llama3.1:8b",        # ‡∏™‡∏°‡∏î‡∏∏‡∏•, RAM ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á (8GB)
    "large": "llama3.1:70b",          # ‡πÉ‡∏´‡∏ç‡πà, ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á (80GB)
    "coding": "codellama:7b",         # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏Ñ‡πâ‡∏î
    "chat": "mistral:7b",              # ‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤
    "scb10x": "llama3.2-typhoon2-1b-instruct:latest"
}


# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
if __name__ == "__main__":
    # ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡πÇ‡∏°‡∏î‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
    from vector_store import ThaiVectorStoreManager
    from document_processor import ThaiDocumentProcessor
    from langchain.schema import Document
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    test_docs = [
        Document(
            page_content="‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏°‡∏µ‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏´‡∏•‡∏ß‡∏á‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£ ‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 70 ‡∏•‡πâ‡∏≤‡∏ô‡∏Ñ‡∏ô",
            metadata={"source": "thailand_info.txt", "topic": "general"}
        ),
        Document(
            page_content="‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà ‡∏ï‡πâ‡∏°‡∏¢‡∏≥‡∏Å‡∏∏‡πâ‡∏á ‡∏ú‡∏±‡∏î‡πÑ‡∏ó‡∏¢ ‡πÅ‡∏•‡∏∞‡∏°‡∏∞‡∏°‡πà‡∏ß‡∏á‡∏Ç‡πâ‡∏≤‡∏ß‡πÄ‡∏´‡∏ô‡∏µ‡∏¢‡∏ß",
            metadata={"source": "thai_food.txt", "topic": "food"}
        )
    ]
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á vector store
    vector_manager = ThaiVectorStoreManager()
    vector_manager.add_documents(test_docs)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á RAG system
    rag_system = ThaiRAGSystem(
        vector_store_manager=vector_manager,
        llm_model="llama3.1:8b",  # ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
        temperature=0.1
    )
    
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
    questions = [
        "‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏´‡∏•‡∏ß‡∏á‡∏Ç‡∏≠‡∏á‡πÑ‡∏ó‡∏¢‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?",
        "‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏£‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà?",
        "‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?",
        "‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏û‡∏π‡∏î‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ ‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏´‡∏•‡∏ß‡∏á‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏£‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà?"  # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö chat history
    ]
    
    print("ü§ñ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏î‡∏™‡∏≠‡∏ö RAG System")
    print("=" * 50)
    
    for question in questions:
        result = rag_system.ask_question(question)
        
        print(f"\n‚ùì ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {result['question']}")
        print(f"ü§ñ ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {result['answer']}")
        print(f"üìö ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {len(result['sources'])}")
        
        if result['sources']:
            print("üìñ ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:")
            for i, source in enumerate(result['sources'], 1):
                print(f"   {i}. {source['filename']} (chunk {source['chunk_index']})")
        
        print("-" * 30)
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤
    print("\nüí¨ ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤:")
    history = rag_system.get_chat_history()
    for i, chat in enumerate(history, 1):
        print(f"{i}. Q: {chat['question']}")
        print(f"   A: {chat['answer'][:100]}...")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
    stats = rag_system.get_system_stats()
    print(f"\nüìä ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏£‡∏∞‡∏ö‡∏ö: {stats}")