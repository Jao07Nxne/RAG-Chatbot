"""
RAG System for Thai Chatbot with Chat History Support
‡∏£‡∏∞‡∏ö‡∏ö RAG ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÅ‡∏ö‡∏ö‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ Local LLM
"""

import os
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

from langchain.schema import Document, BaseMessage, HumanMessage, AIMessage
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate
from langchain.llms.base import LLM
from langchain.callbacks.manager import CallbackManagerForLLMRun

import ollama
import requests


class OllamaLLM(LLM):
    """Custom LLM class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Ollama"""
    
    model_name: str = "llama3.1"
    temperature: float = 0.1
    max_tokens: int = 2048
    
    def __init__(self, model_name: str = "llama3.1", temperature: float = 0.1, **kwargs):
        super().__init__(**kwargs)
        self.model_name = model_name
        self.temperature = temperature
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Ollama
        self._check_ollama_connection()
    
    def _check_ollama_connection(self) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Ollama"""
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                available_models = [model['name'] for model in models]
                
                if self.model_name not in available_models:
                    print(f"‚ö†Ô∏è ‡πÇ‡∏°‡πÄ‡∏î‡∏• {self.model_name} ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ô Ollama")
                    print(f"üìã ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà: {available_models}")
                    if available_models:
                        print(f"üîÑ ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• {available_models[0]} ‡πÅ‡∏ó‡∏ô")
                        self.model_name = available_models[0]
                
                print(f"‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Ollama ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à - ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•: {self.model_name}")
                return True
            else:
                raise Exception(f"HTTP {response.status_code}")
                
        except Exception as e:
            print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Ollama: {e}")
            print("üí° ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Ollama ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà (ollama serve)")
            return False
    
    def _call(self, prompt: str, stop: Optional[List[str]] = None, 
              run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs) -> str:
        """‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Ollama LLM"""
        try:
            response = ollama.chat(
                model=self.model_name,
                messages=[
                    {
                        "role": "system", 
                        "content": "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏ï‡∏≠‡∏ö‡∏™‡∏±‡πâ‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡∏Ñ‡∏≥ ‡∏´‡πâ‡∏≤‡∏°‡∏û‡∏π‡∏î‡πÄ‡∏¢‡∏¥‡πà‡∏ô‡πÄ‡∏¢‡πâ‡∏≠"
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                options={
                    "temperature": self.temperature,
                    "num_predict": self.max_tokens,  # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡∏à‡∏≤‡∏Å UI ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
                    "top_p": 0.9,
                    "top_k": 40,  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î choices
                    "repeat_penalty": 1.1,  # ‡∏•‡∏î‡∏Ñ‡πà‡∏≤ penalty - ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÉ‡∏´‡πâ‡∏ã‡πâ‡∏≥‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£)
                    "stop": [
                        "</s>", 
                        "<|end|>", 
                        "Human:", 
                        "\n\n‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:",  # ‡∏´‡∏¢‡∏∏‡∏î‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà
                        "\n\n---\n"  # ‡∏´‡∏¢‡∏∏‡∏î‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠ separator
                    ]
                }
            )
            
            result = response['message']['content']
            
            # ‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏≠‡∏≠‡∏Å (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ã‡πâ‡∏≥‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 50 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)
            lines = result.split('\n')
            seen = set()
            unique_lines = []
            for line in lines:
                line_stripped = line.strip()
                if len(line_stripped) > 20:  # ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
                    if line_stripped not in seen:
                        unique_lines.append(line)
                        seen.add(line_stripped)
                else:
                    unique_lines.append(line)
            
            result = '\n'.join(unique_lines)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
            if not result or len(result.strip()) < 5:
                return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÑ‡∏î‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á"
            
            # ‡∏ï‡∏±‡∏î‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô‡∏ñ‡πâ‡∏≤‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (max 1000 chars)
            if len(result) > 1000:
                result = result[:997] + "..."
            
            return result.strip()
            
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM: {e}")
            return f"‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•: {str(e)}"
    
    @property
    def _llm_type(self) -> str:
        return "ollama"


class ThaiRAGSystem:
    """‡∏£‡∏∞‡∏ö‡∏ö RAG ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏û‡∏£‡πâ‡∏≠‡∏° Chat History"""
    
    def __init__(self, 
                 vector_store_manager,
                 llm_model: str = "llama3.1",
                 temperature: float = 0.1,
                 max_tokens: int = 2048):
        """
        Args:
            vector_store_manager: ThaiVectorStoreManager instance
            llm_model: ‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏• Ollama ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ
            temperature: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö (0-1)
            max_tokens: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö
        """
        self.vector_store_manager = vector_store_manager
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á LLM
        self.llm = OllamaLLM(
            model_name=llm_model,
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Memory ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt Template
        self.prompt_template = self._create_prompt_template()
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á RAG Chain
        self.rag_chain = None
        self._setup_rag_chain()
    
    def _create_prompt_template(self) -> PromptTemplate:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt Template ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"""
        template = """‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢

‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£:
{context}

‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤:
{chat_history}

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}

‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö (‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ñ‡∏£‡πà‡∏á‡∏Ñ‡∏£‡∏±‡∏î):

**‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô "05506012 ‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏¥‡∏ä‡∏≤‡∏≠‡∏∞‡πÑ‡∏£"):**
- ‚úÖ ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤
- ‚úÖ ‡∏ï‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢: "‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤ [‡∏£‡∏´‡∏±‡∏™] ‡∏Ñ‡∏∑‡∏≠ [‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡∏ä‡∏≤‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢] ([‡∏ä‡∏∑‡πà‡∏≠‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏ñ‡πâ‡∏≤‡∏°‡∏µ]) ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô [‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï] ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï"
- ‚ùå ‡∏´‡πâ‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏•‡∏Ç‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡πá‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡∏ä‡∏≤

**‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤‡πÉ‡∏ô‡∏†‡∏≤‡∏Ñ/‡∏õ‡∏µ (‡πÄ‡∏ä‡πà‡∏ô "‡∏õ‡∏µ‡∏ó‡∏µ‡πà 1 ‡∏†‡∏≤‡∏Ñ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏µ‡πà 1 ‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á"):**
- ‚úÖ **‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡∏ä‡∏≤‡∏Ñ‡∏£‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î 100%**
- ‚úÖ ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ bullet point ‡∏ó‡∏∏‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤:
  ‚Ä¢ ‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤ ‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡∏ä‡∏≤ (‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï)
  ‚Ä¢ ‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤ ‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡∏ä‡∏≤ (‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï)
  ‚Ä¢ (‡πÅ‡∏™‡∏î‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏à‡∏ô‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤)
- ‚úÖ ‡∏ö‡∏≠‡∏Å‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏ß‡∏°‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏ó‡πâ‡∏≤‡∏¢‡∏™‡∏∏‡∏î
- ‚ùå **‡∏´‡πâ‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡πÅ‡∏Ñ‡πà‡∏ö‡∏≤‡∏á‡∏ß‡∏¥‡∏ä‡∏≤** - ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏∏‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
- ‚ùå **‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ "..." ‡∏´‡∏£‡∏∑‡∏≠ "‡πÅ‡∏•‡∏∞‡∏≠‡∏∑‡πà‡∏ô‡πÜ"** - ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ñ‡∏£‡∏ö

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á:**
"‡πÉ‡∏ô‡∏õ‡∏µ‡∏ó‡∏µ‡πà 1 ‡∏†‡∏≤‡∏Ñ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏µ‡πà 2 ‡∏°‡∏µ‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:

‚Ä¢ 05506233 ‡πÅ‡∏Ñ‡∏•‡∏Ñ‡∏π‡∏•‡∏±‡∏™‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå (3 ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï)
‚Ä¢ 05506001 ‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏î‡∏¥‡∏™‡∏Ñ‡∏£‡∏µ‡∏ï (3 ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï)
‚Ä¢ 05506004 ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡πÄ‡∏ä‡∏¥‡∏á‡∏≠‡∏≠‡∏ö‡πÄ‡∏à‡∏Å‡∏ï‡πå (3 ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï)
‚Ä¢ 05506008 ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå (3 ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï)
‚Ä¢ 05506011 ‡∏õ‡∏è‡∏¥‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå (3 ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï)
‚Ä¢ 90644008 ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô 2 (3 ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï)
‚Ä¢ 90xxxxxx ‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏°‡∏ß‡∏î‡∏ß‡∏¥‡∏ä‡∏≤‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ (3 ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï)

‡∏£‡∏ß‡∏° 21 ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï"

**‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ:**
- ‚úÖ ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
- ‚úÖ ‡∏ï‡∏≠‡∏ö‡∏™‡∏±‡πâ‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö 3-5 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
- ‚ùå ‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏ï‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏´‡πâ‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡∏Ñ‡∏≥
- ‚ùå ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‚Üí ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£"

‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö (‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô):"""
        
        return PromptTemplate(
            template=template,
            input_variables=["context", "chat_history", "question"]
        )
    
    def _setup_rag_chain(self):
        """‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ RAG Chain"""
        if self.vector_store_manager.vector_store is None:
            print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ vector store ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á RAG chain")
            return
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á retriever
        retriever = self.vector_store_manager.get_retriever(k=5)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á ConversationalRetrievalChain
        self.rag_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=retriever,
            memory=self.memory,
            return_source_documents=True,
            combine_docs_chain_kwargs={"prompt": self.prompt_template},
            verbose=True
        )
        
        print("‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á RAG Chain ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    
    def ask_question(self, question: str) -> Dict[str, Any]:
        """
        ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        
        Args:
            question: ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
            
        Returns:
            Dict containing answer, sources, and metadata
        """
        if self.rag_chain is None:
            return {
                "answer": "‡∏£‡∏∞‡∏ö‡∏ö‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏Å‡πà‡∏≠‡∏ô",
                "sources": [],
                "question": question,
                "timestamp": datetime.now().isoformat()
            }
        
        try:
            print(f"ü§î ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ vector store ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if not self.vector_store_manager.vector_store:
                return {
                    "answer": "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏Å‡πà‡∏≠‡∏ô",
                    "sources": [],
                    "question": question,
                    "timestamp": datetime.now().isoformat()
                }
            
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á - ‡πÄ‡∏û‡∏¥‡πà‡∏° k ‡πÄ‡∏õ‡πá‡∏ô 10 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤
            relevant_docs = self.vector_store_manager.vector_store.similarity_search(
                question, 
                k=10  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏õ‡πá‡∏ô 10 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á
            )
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á context
            if not relevant_docs:
                return {
                    "answer": "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î",
                    "sources": [],
                    "question": question,
                    "timestamp": datetime.now().isoformat()
                }
            
            # üéØ ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (Metadata Filtering)
            import re
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á 05506231 ‡πÅ‡∏•‡∏∞ 0550 6231)
            question_course_code = re.search(r'\b(\d{4})\s*(\d{4})\b|\b(\d{8})\b', question)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ä‡∏±‡πâ‡∏ô‡∏õ‡∏µ/‡∏†‡∏≤‡∏Ñ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
            question_year = re.search(r'‡∏õ‡∏µ‡∏ó‡∏µ‡πà\s*(\d+)', question)
            question_semester = re.search(r'‡∏†‡∏≤‡∏Ñ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏µ‡πà\s*(\d+)', question)
            
            filtered_docs = []
            
            # üîç ‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏° metadata ‡πÅ‡∏•‡∏∞ content ‡∏ñ‡πâ‡∏≤‡∏û‡∏ö‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
            for doc in relevant_docs:
                metadata = doc.metadata
                content = doc.page_content
                
                # ‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≤‡∏°‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‚Üí ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏£‡∏´‡∏±‡∏™‡∏ô‡∏±‡πâ‡∏ô‡πÉ‡∏ô content (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á)
                if question_course_code:
                    # ‡∏î‡∏∂‡∏á‡∏£‡∏´‡∏±‡∏™‡∏à‡∏≤‡∏Å regex (group 1+2 ‡∏´‡∏£‡∏∑‡∏≠ group 3)
                    if question_course_code.group(3):  # 8 ‡∏´‡∏•‡∏±‡∏Å‡∏ï‡∏¥‡∏î
                        target_code = question_course_code.group(3)
                    else:  # 4+4 ‡∏´‡∏•‡∏±‡∏Å‡πÅ‡∏¢‡∏Å
                        target_code = question_course_code.group(1) + question_course_code.group(2)
                    
                    # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö: "05506231" ‡∏´‡∏£‡∏∑‡∏≠ "0550 6231" ‡∏´‡∏£‡∏∑‡∏≠ "0550-6231"
                    pattern1 = target_code  # ‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô
                    pattern2 = target_code[:4] + r'\s+' + target_code[4:]  # ‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á
                    pattern3 = target_code[:4] + r'\s*-\s*' + target_code[4:]  # ‡∏°‡∏µ‡∏Ç‡∏µ‡∏î
                    
                    if (target_code in content or 
                        re.search(pattern2, content) or
                        re.search(pattern3, content) or
                        target_code in metadata.get("course_codes", "")):
                        # ‡∏û‡∏ö‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤! priority ‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å
                        filtered_docs.insert(0, doc)
                        print(f"‚úÖ ‡∏û‡∏ö‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤ {target_code} ‡πÉ‡∏ô chunk: {content[:100]}")
                        continue
                
                # ‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≤‡∏°‡∏õ‡∏µ/‡∏†‡∏≤‡∏Ñ ‚Üí ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏±‡πâ‡∏á metadata ‡πÅ‡∏•‡∏∞ content ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
                elif question_year or question_semester:
                    match_score = 0  # ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏£‡∏á
                    
                    # ‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≤‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏õ‡∏µ‡πÅ‡∏•‡∏∞‡∏†‡∏≤‡∏Ñ
                    if question_year and question_semester:
                        year_str = question_year.group(1)
                        semester_str = question_semester.group(1)
                        
                        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö metadata ‡∏Å‡πà‡∏≠‡∏ô (‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
                        meta_year_match = metadata.get("year") == year_str
                        meta_semester_match = metadata.get("semester") == semester_str
                        
                        if meta_year_match and meta_semester_match:
                            match_score = 100  # ‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏ö‡∏ö!
                        elif meta_year_match or meta_semester_match:
                            match_score = 50  # ‡∏ï‡∏£‡∏á‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô
                        
                        # ‡∏ñ‡πâ‡∏≤ metadata ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á ‚Üí ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô content ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
                        else:
                            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏õ‡∏µ‡∏ó‡∏µ‡πà X ‡∏†‡∏≤‡∏Ñ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏µ‡πà Y" ‡πÉ‡∏ô content ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                            year_in_content = re.search(rf'‡∏õ‡∏µ‡∏ó‡∏µ‡πà\s*{year_str}', content)
                            semester_in_content = re.search(rf'‡∏†‡∏≤‡∏Ñ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏µ‡πà\s*{semester_str}', content)
                            
                            if year_in_content and semester_in_content:
                                match_score = 80  # ‡πÄ‡∏à‡∏≠‡πÉ‡∏ô content ‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏π‡πà
                            elif year_in_content or semester_in_content:
                                match_score = 40  # ‡πÄ‡∏à‡∏≠‡πÉ‡∏ô content ‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô
                        
                        # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô ‚Üí ‡πÄ‡∏≠‡∏≤‡∏°‡∏≤
                        if match_score > 0:
                            # Priority ‡∏™‡∏π‡∏á‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á
                            if match_score >= 80:
                                filtered_docs.insert(0, doc)
                                print(f"‚úÖ [‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô {match_score}] ‡∏û‡∏ö ‡∏õ‡∏µ{year_str} ‡∏†‡∏≤‡∏Ñ{semester_str}: {content[:100]}")
                            else:
                                filtered_docs.append(doc)
                                print(f"‚ö†Ô∏è [‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô {match_score}] ‡∏û‡∏ö‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô ‡∏õ‡∏µ{year_str} ‡∏†‡∏≤‡∏Ñ{semester_str}")
                            continue
                    
                    # ‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≤‡∏°‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏õ‡∏µ ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏†‡∏≤‡∏Ñ
                    elif question_year:
                        year_str = question_year.group(1)
                        if (metadata.get("year") == year_str or 
                            re.search(rf'‡∏õ‡∏µ‡∏ó‡∏µ‡πà\s*{year_str}', content)):
                            filtered_docs.append(doc)
                            print(f"‚úÖ ‡∏û‡∏ö ‡∏õ‡∏µ{year_str}")
                            continue
                    elif question_semester:
                        semester_str = question_semester.group(1)
                        if (metadata.get("semester") == semester_str or 
                            re.search(rf'‡∏†‡∏≤‡∏Ñ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏µ‡πà\s*{semester_str}', content)):
                            filtered_docs.append(doc)
                            print(f"‚úÖ ‡∏û‡∏ö ‡∏†‡∏≤‡∏Ñ{semester_str}")
                            continue
                
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏û‡∏¥‡πÄ‡∏®‡∏© ‚Üí ‡πÄ‡∏≠‡∏≤‡∏ó‡∏∏‡∏Å doc
                else:
                    filtered_docs.append(doc)
            
            # ‡∏ñ‡πâ‡∏≤‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏•‡∏¢ ‚Üí ‡πÉ‡∏ä‡πâ docs ‡πÄ‡∏î‡∏¥‡∏°
            if not filtered_docs:
                filtered_docs = relevant_docs
                print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(relevant_docs)} chunks")
            else:
                print(f"‚úÖ ‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {len(filtered_docs)} chunks ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç")
            
            # Deduplication
            unique_docs = []
            seen_contents = set()
            for doc in filtered_docs:
                # ‡πÉ‡∏ä‡πâ 100 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÅ‡∏£‡∏Å‡πÄ‡∏õ‡πá‡∏ô signature
                signature = doc.page_content[:100].strip()
                if signature not in seen_contents:
                    unique_docs.append(doc)
                    seen_contents.add(signature)
            
            # ‡πÉ‡∏ä‡πâ top 2 chunks (‡∏•‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Ollama ‡∏•‡πà‡∏°)
            num_chunks = 5 if (question_year or question_semester or "‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á" in question or "‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î" in question) else 3
            relevant_docs = unique_docs[:num_chunks]
            
            # üî• ‡∏™‡∏£‡πâ‡∏≤‡∏á context ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ï‡πá‡∏° (‡πÑ‡∏°‡πà‡∏ï‡∏±‡∏î) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á
            # gemma2:2b ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö ~2048 tokens ‚âà 1500-1800 chars
            context_parts = []
            total_chars = 0
            max_context_chars = 2000  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            
            for doc in relevant_docs:
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ß‡πà‡∏≤‡∏¢‡∏±‡∏á‡πÉ‡∏™‡πà‡πÑ‡∏î‡πâ‡∏≠‡∏µ‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà
                remaining = max_context_chars - total_chars
                
                if remaining <= 0:
                    break
                
                # ‡πÉ‡∏ä‡πâ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ï‡πá‡∏°‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô limit
                content = doc.page_content[:remaining]
                
                # ‡πÄ‡∏û‡∏¥‡πà‡∏° metadata hint ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
                metadata_hint = ""
                if doc.metadata.get("year") and doc.metadata.get("semester"):
                    metadata_hint = f"[‡∏õ‡∏µ‡∏ó‡∏µ‡πà {doc.metadata['year']} ‡∏†‡∏≤‡∏Ñ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏µ‡πà {doc.metadata['semester']}]\n"
                
                context_parts.append(metadata_hint + content)
                total_chars += len(metadata_hint) + len(content)
            
            context = "\n\n---\n\n".join(context_parts)
            
            print(f"ÔøΩ ‡πÉ‡∏ä‡πâ context ‡∏£‡∏ß‡∏° {total_chars} chars ‡∏à‡∏≤‡∏Å {len(context_parts)} chunks")
            
            # Debug: ‡πÅ‡∏™‡∏î‡∏á context
            print(f"üìÑ Context ‡∏ó‡∏µ‡πà‡∏û‡∏ö ({len(context)} chars): {context[:200]}...")
            
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ RAG Chain ‡∏î‡πâ‡∏ß‡∏¢ ConversationalRetrievalChain
            result = self.rag_chain({
                "question": question,
                "chat_history": self.memory.chat_memory.messages
            })
            
            # ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            sources = []
            seen_sources = set()
            for doc in relevant_docs:
                source_info = {
                    "content": doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content,
                    "metadata": doc.metadata,
                    "filename": doc.metadata.get("filename", "‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö"),
                    "chunk_index": doc.metadata.get("chunk_index", 0)
                }
                
                # ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡∏ã‡πâ‡∏≥
                source_key = f"{source_info['filename']}_{source_info['chunk_index']}"
                if source_key not in seen_sources:
                    sources.append(source_info)
                    seen_sources.add(source_key)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
            answer = result.get("answer", result.get("text", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ"))
            
            # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
            if len(answer.strip()) < 10:
                answer = "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÑ‡∏î‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°"
            
            print(f"‚úÖ ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {answer[:100]}...")
            
            return {
                "answer": answer,
                "sources": sources,
                "question": question,
                "timestamp": datetime.now().isoformat(),
                "chat_history_length": len(self.memory.chat_memory.messages)
            }
            
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {e}")
            return {
                "answer": f"‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}",
                "sources": [],
                "question": question,
                "timestamp": datetime.now().isoformat()
            }
    
    def get_chat_history(self) -> List[Dict[str, str]]:
        """‡∏î‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤"""
        history = []
        messages = self.memory.chat_memory.messages
        
        for i in range(0, len(messages), 2):
            if i + 1 < len(messages):
                human_msg = messages[i]
                ai_msg = messages[i + 1]
                
                history.append({
                    "question": human_msg.content,
                    "answer": ai_msg.content,
                    "timestamp": datetime.now().isoformat()  # ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÄ‡∏Å‡πá‡∏ö timestamp ‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô metadata
                })
        
        return history
    
    def clear_chat_history(self):
        """‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤"""
        self.memory.clear()
        print("‚úÖ ‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
    
    def update_vector_store(self, vector_store_manager):
        """‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó vector store ‡πÅ‡∏•‡∏∞ RAG chain"""
        self.vector_store_manager = vector_store_manager
        self._setup_rag_chain()
        print("‚úÖ ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó vector store ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
    
    def get_system_stats(self) -> Dict[str, Any]:
        """‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö"""
        vector_stats = self.vector_store_manager.get_stats()
        
        return {
            "vector_store": vector_stats,
            "llm_model": self.llm.model_name,
            "chat_history_length": len(self.memory.chat_memory.messages) // 2,
            "rag_chain_ready": self.rag_chain is not None,
            "timestamp": datetime.now().isoformat()
        }


# ‡πÇ‡∏°‡πÄ‡∏î‡∏• LLM ‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
RECOMMENDED_THAI_LLM_MODELS = {
    "small_fast": "gemma2:2b",        # ‡πÄ‡∏•‡πá‡∏Å, ‡πÄ‡∏£‡πá‡∏ß, RAM ‡∏ô‡πâ‡∏≠‡∏¢ (4GB) - ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
    "balanced": "llama3.1:8b",        # ‡∏™‡∏°‡∏î‡∏∏‡∏•, RAM ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á (8GB)
    "gemma2":"latest",
    "large": "llama3.1:70b"         # ‡πÉ‡∏´‡∏ç‡πà, ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á (80GB)
    
}


# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
if __name__ == "__main__":
    # ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡πÇ‡∏°‡∏î‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
    from vector_store import ThaiVectorStoreManager
    from document_processor import ThaiDocumentProcessor
    from langchain.schema import Document
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    test_docs = [
        Document(
            page_content="‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏°‡∏µ‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏´‡∏•‡∏ß‡∏á‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£ ‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 70 ‡∏•‡πâ‡∏≤‡∏ô‡∏Ñ‡∏ô",
            metadata={"source": "thailand_info.txt", "topic": "general"}
        ),
        Document(
            page_content="‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà ‡∏ï‡πâ‡∏°‡∏¢‡∏≥‡∏Å‡∏∏‡πâ‡∏á ‡∏ú‡∏±‡∏î‡πÑ‡∏ó‡∏¢ ‡πÅ‡∏•‡∏∞‡∏°‡∏∞‡∏°‡πà‡∏ß‡∏á‡∏Ç‡πâ‡∏≤‡∏ß‡πÄ‡∏´‡∏ô‡∏µ‡∏¢‡∏ß",
            metadata={"source": "thai_food.txt", "topic": "food"}
        )
    ]
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á vector store
    vector_manager = ThaiVectorStoreManager()
    vector_manager.add_documents(test_docs)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á RAG system
    rag_system = ThaiRAGSystem(
        vector_store_manager=vector_manager,
        llm_model="llama3.1:8b",  # ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
        temperature=0.1
    )
    
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
    questions = [
        "‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏´‡∏•‡∏ß‡∏á‡∏Ç‡∏≠‡∏á‡πÑ‡∏ó‡∏¢‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?",
        "‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏£‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà?",
        "‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?",
        "‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏û‡∏π‡∏î‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ ‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏´‡∏•‡∏ß‡∏á‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏£‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà?"  # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö chat history
    ]
    
    print("ü§ñ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏î‡∏™‡∏≠‡∏ö RAG System")
    print("=" * 50)
    
    for question in questions:
        result = rag_system.ask_question(question)
        
        print(f"\n‚ùì ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {result['question']}")
        print(f"ü§ñ ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {result['answer']}")
        print(f"üìö ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {len(result['sources'])}")
        
        if result['sources']:
            print("üìñ ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:")
            for i, source in enumerate(result['sources'], 1):
                print(f"   {i}. {source['filename']} (chunk {source['chunk_index']})")
        
        print("-" * 30)
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤
    print("\nüí¨ ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤:")
    history = rag_system.get_chat_history()
    for i, chat in enumerate(history, 1):
        print(f"{i}. Q: {chat['question']}")
        print(f"   A: {chat['answer'][:100]}...")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
    stats = rag_system.get_system_stats()
    print(f"\nüìä ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏£‡∏∞‡∏ö‡∏ö: {stats}")