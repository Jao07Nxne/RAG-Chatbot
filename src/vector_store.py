"""
Vector Store Manager for Thai RAG Chatbot
‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ embeddings ‡πÅ‡∏•‡∏∞ vector database ‡∏î‡πâ‡∏ß‡∏¢ FAISS ‡πÅ‡∏•‡∏∞ local embedding models
"""

import os
import pickle
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path

import numpy as np
import faiss
from sentence_transformers import SentenceTransformer

from langchain.schema import Document
from langchain.vectorstores import FAISS
from langchain.embeddings.base import Embeddings


class LocalThaiEmbeddings(Embeddings):
    """
    Local Embedding class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
    ‡πÉ‡∏ä‡πâ Sentence Transformers models ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
    """
    
    def __init__(self, model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"):
        """
        Args:
            model_name: ‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏• embedding ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ
                       ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:
                       - paraphrase-multilingual-MiniLM-L12-v2 (‡πÄ‡∏£‡πá‡∏ß, ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤)
                       - distiluse-base-multilingual-cased (‡∏î‡∏µ, ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤)
                       - paraphrase-multilingual-mpnet-base-v2 (‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î, ‡∏ä‡πâ‡∏≤‡∏´‡∏ô‡πà‡∏≠‡∏¢)
        """
        self.model_name = model_name
        print(f"üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• embedding: {model_name}")
        
        try:
            self.model = SentenceTransformer(model_name)
            print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• {model_name} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        except Exception as e:
            print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• {model_name}: {e}")
            # ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏£‡∏≠‡∏á
            backup_model = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
            print(f"üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏£‡∏≠‡∏á: {backup_model}")
            self.model = SentenceTransformer(backup_model)
            self.model_name = backup_model
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
        try:
            embeddings = self.model.encode(texts, convert_to_tensor=False)
            return embeddings.tolist()
        except Exception as e:
            print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings: {e}")
            raise
    
    def embed_query(self, text: str) -> List[float]:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°"""
        try:
            embedding = self.model.encode([text], convert_to_tensor=False)
            return embedding[0].tolist()
        except Exception as e:
            print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {e}")
            raise


class ThaiVectorStoreManager:
    """‡∏Ñ‡∏•‡∏≤‡∏™‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Vector Store ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"""
    
    def __init__(self, 
                 embedding_model: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                 vector_store_path: str = "./vectorstore"):
        """
        Args:
            embedding_model: ‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏• embedding
            vector_store_path: ‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡πÄ‡∏Å‡πá‡∏ö vector store
        """
        self.embedding_model_name = embedding_model
        self.vector_store_path = Path(vector_store_path)
        self.vector_store_path.mkdir(exist_ok=True)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding model
        self.embeddings = LocalThaiEmbeddings(embedding_model)
        
        # Vector store
        self.vector_store: Optional[FAISS] = None
        self.documents: List[Document] = []
        
        # Metadata ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£
        self.metadata_path = self.vector_store_path / "metadata.pkl"
        self.faiss_index_path = self.vector_store_path / "index.faiss"
        self.faiss_pkl_path = self.vector_store_path / "index.pkl"
    
    def add_documents(self, documents: List[Document]) -> None:
        """‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏•‡∏á‡πÉ‡∏ô vector store"""
        if not documents:
            print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°")
            return
        
        print(f"üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏° {len(documents)} ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏•‡∏á‡πÉ‡∏ô vector store...")
        
        # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡∏≠‡∏≠‡∏Å (deduplication ‡∏Å‡πà‡∏≠‡∏ô embed)
        unique_docs = []
        seen_contents = set()
        
        for doc in documents:
            # ‡πÉ‡∏ä‡πâ content ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏õ‡πá‡∏ô signature
            content_hash = hash(doc.page_content.strip())
            if content_hash not in seen_contents:
                unique_docs.append(doc)
                seen_contents.add(content_hash)
            else:
                print(f"  ‚è≠Ô∏è ‡∏Ç‡πâ‡∏≤‡∏° chunk ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥ (hash: {content_hash})")
        
        print(f"  ‚úÇÔ∏è ‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß: {len(documents)} ‚Üí {len(unique_docs)} ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ (‡∏•‡∏ö‡∏ã‡πâ‡∏≥ {len(documents) - len(unique_docs)} ‡∏ä‡∏¥‡πâ‡∏ô)")
        
        if not unique_docs:
            print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÉ‡∏´‡∏°‡πà‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á")
            return
        
        if self.vector_store is None:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á vector store ‡πÉ‡∏´‡∏°‡πà
            self.vector_store = FAISS.from_documents(unique_docs, self.embeddings)
            self.documents = unique_docs.copy()
        else:
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏•‡∏á‡πÉ‡∏ô vector store ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
            self.vector_store.add_documents(unique_docs)
            self.documents.extend(unique_docs)
        
        print(f"‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô ‡∏£‡∏ß‡∏° {len(self.documents)} ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£")
    
    def save_vector_store(self) -> None:
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å vector store ‡∏•‡∏á‡∏î‡∏¥‡∏™‡∏Å‡πå"""
        if self.vector_store is None:
            print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ vector store ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")
            return
        
        try:
            print("üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å vector store...")
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å FAISS index
            self.vector_store.save_local(str(self.vector_store_path))
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å metadata
            metadata = {
                "embedding_model": self.embedding_model_name,
                "total_documents": len(self.documents),
                "documents": self.documents
            }
            
            with open(self.metadata_path, 'wb') as f:
                pickle.dump(metadata, f)
            
            print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å vector store ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô ‡∏ó‡∏µ‡πà {self.vector_store_path}")
            
        except Exception as e:
            print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å vector store: {e}")
            raise
    
    def load_vector_store(self) -> bool:
        """‡πÇ‡∏´‡∏•‡∏î vector store ‡∏à‡∏≤‡∏Å‡∏î‡∏¥‡∏™‡∏Å‡πå"""
        try:
            if not self.faiss_index_path.exists() or not self.metadata_path.exists():
                print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö vector store ‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ")
                return False
            
            print("üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î vector store...")
            
            # ‡πÇ‡∏´‡∏•‡∏î metadata
            with open(self.metadata_path, 'rb') as f:
                metadata = pickle.load(f)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö embedding model
            if metadata['embedding_model'] != self.embedding_model_name:
                print(f"‚ö†Ô∏è Embedding model ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô: {metadata['embedding_model']} vs {self.embedding_model_name}")
                print("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏î‡πâ‡∏ß‡∏¢ embedding model ‡πÉ‡∏´‡∏°‡πà...")
            
            # ‡πÇ‡∏´‡∏•‡∏î FAISS vector store
            self.vector_store = FAISS.load_local(
                str(self.vector_store_path), 
                self.embeddings,
                allow_dangerous_deserialization=True
            )
            
            self.documents = metadata['documents']
            
            print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î vector store ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô - {len(self.documents)} ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£")
            return True
            
        except Exception as e:
            print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î vector store: {e}")
            return False
    
    def search_similar(self, query: str, k: int = 5, score_threshold: float = 0.0) -> List[Tuple[Document, float]]:
        """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°"""
        if self.vector_store is None:
            print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ vector store ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤")
            return []
        
        try:
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏î‡πâ‡∏ß‡∏¢ similarity score
            results = self.vector_store.similarity_search_with_score(query, k=k)
            
            # ‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏° score threshold
            filtered_results = [(doc, score) for doc, score in results if score >= score_threshold]
            
            return filtered_results
            
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤: {e}")
            return []
    
    def get_retriever(self, k: int = 5, search_type: str = "similarity"):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á retriever ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡πÉ‡∏ô RAG"""
        if self.vector_store is None:
            raise ValueError("‡πÑ‡∏°‡πà‡∏°‡∏µ vector store ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á retriever")
        
        return self.vector_store.as_retriever(
            search_type=search_type,
            search_kwargs={"k": k}
        )
    
    def get_stats(self) -> Dict[str, Any]:
        """‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á vector store"""
        if self.vector_store is None:
            return {"status": "empty"}
        
        return {
            "status": "ready",
            "total_documents": len(self.documents),
            "embedding_model": self.embedding_model_name,
            "vector_store_path": str(self.vector_store_path),
            "index_size": self.vector_store.index.ntotal if hasattr(self.vector_store, 'index') else 0
        }
    
    def clear_vector_store(self) -> None:
        """‡∏•‡πâ‡∏≤‡∏á vector store"""
        self.vector_store = None
        self.documents = []
        
        # ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ
        for file_path in [self.faiss_index_path, self.faiss_pkl_path, self.metadata_path]:
            if file_path.exists():
                file_path.unlink()
        
        print("‚úÖ ‡∏•‡πâ‡∏≤‡∏á vector store ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")


# ‡πÇ‡∏°‡πÄ‡∏î‡∏• embedding ‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
RECOMMENDED_THAI_MODELS = {
    "fast": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    "balanced": "sentence-transformers/distiluse-base-multilingual-cased", 
    "best": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
    "thai_specific": "airesearch/wangchanberta-base-att-spm-uncased"  # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
}


# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
if __name__ == "__main__":
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á vector store manager
    manager = ThaiVectorStoreManager(
        embedding_model=RECOMMENDED_THAI_MODELS["fast"],
        vector_store_path="./test_vectorstore"
    )
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    test_docs = [
        Document(
            page_content="‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏°‡∏µ‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏´‡∏•‡∏ß‡∏á‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£",
            metadata={"source": "test1.txt", "topic": "geography"}
        ),
        Document(
            page_content="‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏°‡∏µ‡∏£‡∏™‡∏ä‡∏≤‡∏ï‡∏¥‡∏´‡∏ß‡∏≤‡∏ô ‡πÄ‡∏õ‡∏£‡∏µ‡πâ‡∏¢‡∏ß ‡πÄ‡∏Ñ‡πá‡∏° ‡πÄ‡∏ú‡πá‡∏î",
            metadata={"source": "test2.txt", "topic": "food"}
        ),
        Document(
            page_content="‡∏û‡∏£‡∏∞‡∏ö‡∏£‡∏°‡∏°‡∏´‡∏≤‡∏£‡∏≤‡∏ä‡∏ß‡∏±‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£",
            metadata={"source": "test3.txt", "topic": "landmarks"}
        )
    ]
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
    manager.add_documents(test_docs)
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
    manager.save_vector_store()
    
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
    results = manager.search_similar("‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏´‡∏•‡∏ß‡∏á‡∏Ç‡∏≠‡∏á‡πÑ‡∏ó‡∏¢", k=2)
    print("\nüîç ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤:")
    for doc, score in results:
        print(f"Score: {score:.4f}")
        print(f"Content: {doc.page_content}")
        print(f"Metadata: {doc.metadata}")
        print("-" * 50)
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
    stats = manager.get_stats()
    print(f"\nüìä ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥: {stats}")
    
    # ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    manager.clear_vector_store()
    import shutil
    if Path("./test_vectorstore").exists():
        shutil.rmtree("./test_vectorstore")